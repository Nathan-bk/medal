{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPLadMM9RgWZkJNmgfA3RW2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfXeUfdDN3JQ","executionInfo":{"status":"ok","timestamp":1646385506719,"user_tz":-60,"elapsed":13193,"user":{"displayName":"nathan bigaud","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12543085100510385482"}},"outputId":"354e1dd1-6e98-4fad-f766-105a5365a157"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd drive/MyDrive/work/S2/NLP/medal"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1Sld3r8PXti","executionInfo":{"status":"ok","timestamp":1646385838320,"user_tz":-60,"elapsed":292,"user":{"displayName":"nathan bigaud","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12543085100510385482"}},"outputId":"8bc16720-b54b-4a55-ee96-0ebe116f5f1d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/work/S2/NLP/medal\n"]}]},{"cell_type":"code","source":["%pip install transformers fasttext -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlWa6hNrPtL6","executionInfo":{"status":"ok","timestamp":1646385904945,"user_tz":-60,"elapsed":9373,"user":{"displayName":"nathan bigaud","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12543085100510385482"}},"outputId":"5434aa69-1838-4b82-9a3c-38fb6b8a2122"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 4.9 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.4 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 56.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 37.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 49.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"]}]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8jB7xKsNkyw","executionInfo":{"status":"ok","timestamp":1646386516944,"user_tz":-60,"elapsed":366119,"user":{"displayName":"nathan bigaud","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12543085100510385482"}},"outputId":"046a9b6b-344e-45fd-844d-40682d654f39"},"outputs":[{"output_type":"stream","name":"stdout","text":["configurations:\n","savedir: ./run_logs\n","model: electra\n","data_dir: ./data\n","data_filename: medal.csv\n","adam_path: ./toy_data/valid_adam.txt\n","embs_path: ./data\n","pretrained_model: None\n","use_scheduler: True\n","lr: 2e-06\n","clip: 0\n","dropout: 0.1\n","epochs: 10\n","accum_num: 1\n","save_every: 1\n","eval_every: 200000\n","batchsize: 8\n","hidden_size: 512\n","rnn_layers: 3\n","da_layers: 1\n","ncpu: 4\n","Data loaded\n","Downloading: 100% 226k/226k [00:00<00:00, 3.35MB/s]\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 38.6kB/s]\n","Downloading: 100% 665/665 [00:00<00:00, 945kB/s]\n","Dataset created\n","model: Electra(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n","      (position_embeddings): Embedding(512, 128)\n","      (token_type_embeddings): Embedding(2, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (output): Linear(in_features=256, out_features=24005, bias=True)\n",")\n","Datasets created:\n","\n","Training set: 3000000 samples\n","\n","Validation set: 1000000 samples\n","\n","Start training\n","\n","  0% 0/375000 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  0% 1601/375000 [04:28<17:24:03,  5.96it/s]\n","Traceback (most recent call last):\n","  File \"run.py\", line 148, in <module>\n","    eval_every=args.eval_every, clip=args.clip, writer=writer, accum_num=args.accum_num,\n","  File \"/content/drive/MyDrive/work/S2/NLP/medal/utils.py\", line 129, in train_loop\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 307, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 156, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n","KeyboardInterrupt\n"]}],"source":["!bash run.sh"]},{"cell_type":"code","source":[""],"metadata":{"id":"3XMEzvMMPK89"},"execution_count":null,"outputs":[]}]}